{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d78ab3",
   "metadata": {},
   "source": [
    "# Botón Para Esconder Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b83b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Aprete aqui para visualizar/esconder bloques de codigo.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sacado de https://mljar.com/blog/jupyter-notebook-hide-code/\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Aprete aqui para visualizar/esconder bloques de codigo.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31a811",
   "metadata": {},
   "source": [
    "## Libraries y Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c493fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299638d",
   "metadata": {},
   "source": [
    "# Parte I: Análisis de la base de hogares y cálculo de pobreza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e8d5",
   "metadata": {},
   "source": [
    "1)  Abrimos la base de datos de la EPH y nos quedamos con los observaciones de GBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df0ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrimos la base\n",
    "hogar = pd.read_excel('./EPH_usu_1er_Trim_2022_xlsx/usu_hogar_T122.xlsx')\n",
    "\n",
    "#Eliminamos todos los aglomerados que no correspondan a CABA (cód 32) o GBA (cód 33)\n",
    "hogar_cortado = hogar.query(\"AGLOMERADO in (32, 33)\") # la manera mas rapida y facil de hacerlo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c136c0",
   "metadata": {},
   "source": [
    "2) Unimos encuesta individual y encuesta hogar (en base a CODUDU y NRO HOGAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4cd57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importamos tabla individual \n",
    "#Abrimos la base\n",
    "individual = pd.read_excel('./EPH_usu_1er_Trim_2022_xlsx/usu_individual_T122.xlsx')\n",
    "\n",
    "#Eliminamos todos los aglomerados que no correspondan a CABA (cód 32) o GBA (cód 33)\n",
    "individual_cortado = individual.query(\"AGLOMERADO in (32, 33)\") # la manera mas rapida y facil de hacerlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las intersección entre las dos bases como una lista\n",
    "columnas_duplicadas = set(individual_cortado.columns).intersection(set(hogar_cortado.columns))\n",
    "# Removemos CODUSU y NRO_HOGAR de la lista ya que queremos usarlas para la intersección (y las columnas sobre las que se une\n",
    "# no generan duplicados)\n",
    "columnas_duplicadas.remove(\"CODUSU\")\n",
    "columnas_duplicadas.remove(\"NRO_HOGAR\")\n",
    "\n",
    "# Hacemos el merge habiendo dropeado las columnas repetidas\n",
    "eph = pd.merge(\n",
    "    individual_cortado.drop(columnas_duplicadas, axis=1), \n",
    "    hogar_cortado,\n",
    "    on= ['NRO_HOGAR', 'CODUSU']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4fe945",
   "metadata": {},
   "source": [
    "3) \n",
    "- A continuación, listamos las funciones y procediientos que usamos para limpiar la base:\n",
    "\n",
    "- Para eliminar aquellas observaciones relacionadas a ingreso que sea negativo, utilizaremos el metodo .loc, combinando con el prefijo ~ para expresar que queremos localizar aquellas que no sean negativas en algun punto.\n",
    "-  Eliminamos duplicados con .drop_duplicates\n",
    "- Con .isnull, .dropna y .shape eliminamos aquellas columnas que tengan mas de un porcentaje de NAs.\n",
    "- Armando un interquantile range score eliminamos outliers que en cierto rango.\n",
    "- Para Boxplots, seaborn y matplotlib.\n",
    "- Con .dtypes nos fijamos el tipo de las variables restantes.\n",
    "- Categoricas reemplazamos NaN por metodos .fillna, .isnull, .add_categories.\n",
    "- Continuas relacionadas a ingreso con NAs, asumimos que la no respuesta es un 0 en caso de que no venga ya asi la base.\n",
    "- Eliminaremos outliers utilizando interquantile range scores para detectarlos y eliminarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c78f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Eliminamos aquellas que tengan ingreso o edad negativa\n",
    "eph = eph.loc[~(\n",
    "(eph['CH06']<0) | \n",
    "(eph['PP08D1']<0) | \n",
    "(eph['PP08D4']<0) |\n",
    "(eph['PP08F1']<0) |\n",
    "(eph['PP08F2']<0) |\n",
    "(eph['PP08J1']<0) |\n",
    "(eph['PP08J2']<0) |\n",
    "(eph['PP08J3']<0) |\n",
    "(eph['ITF']<0) |\n",
    "(eph['IPCF']<0) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7973a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Chequeamos si hay duplicados y los dropeamos\n",
    "eph = eph.drop_duplicates()\n",
    "#no hay duplicados, cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheque columnas con muchos NAs, si tiene mas de 70% los vamos a  borrar \n",
    "#Notese quiebre de NAs en 96% y 81%, no es tan arbitrario el 70%\n",
    "percent_missing = eph.isnull().sum() * 100 / len(eph)\n",
    "missing_value_eph = pd.DataFrame({'Columna': eph.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "missing_value_eph.sort_values('Porcentaje de NAs', inplace=True,ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908a2e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Borramos mas de 80%\n",
    "perc = 70.0\n",
    "min_count =  int(((100-perc)/100)*eph.shape[0] + 1)\n",
    "eph = eph.dropna( axis=1, \n",
    "                thresh=min_count)\n",
    "#chequeamos haber borrado lo que queriamos\n",
    "percent_missing = eph.isnull().sum() * 100 / len(eph)\n",
    "missing_value_eph = pd.DataFrame({'Columna': eph.columns,\n",
    "                                 'Porcentaje de NAs': percent_missing})\n",
    "missing_value_eph.sort_values('Porcentaje de NAs', inplace=True,ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc33f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Outliers: Nos quedaremos con los valores de ingreso entre los cuantiles\n",
    "#siguiendo el metodo de interquantile range score propuesto en https://www.pluralsight.com/guides/cleaning-up-data-from-outliers\n",
    "\n",
    "variables_para_outliers=[\n",
    "'PP08D1','PP08D4','PP08F1','PP08F2','PP08J1','PP08J2','PP08J3','ITF','IPCF',\n",
    "'P21', 'TOT_P12', 'P47T'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2273732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots de variables que chequeamos outliers\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(eph[variables_para_outliers]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d531f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generamos Interquantile Ranges, con eso sacamos los que queden afuera\n",
    "#Asimetrico por ser ingreso una variable con lower bound pero no upper bound\n",
    "Qlow = eph[variables_para_outliers].quantile(0)\n",
    "Qhigh = eph[variables_para_outliers].quantile(0.90)\n",
    "IQR = Qhigh - Qlow\n",
    "print(IQR)\n",
    "eph_inc = eph[variables_para_outliers]\n",
    "eph_out = eph_inc[~((eph_inc < (Qlow - 1.5 * IQR)) |(eph_inc > (Qhigh + 1.5 * IQR))).any(axis=1)]\n",
    "indexes = eph_out.index.values.tolist() \n",
    "eph=eph.loc[indexes]\n",
    "print('No es sorpresa que sean 0 las que en graficos no tienen outliers faciles de observar a ojo. Notar que todas con outliers para arriba, no borramos los 0, mas en variables donde sean NO RESPONDE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "eph['ITF'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c01db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos tipo de las variable restantes\n",
    "eph.info(verbose=True)\n",
    "print(\"Son todas object, int64 o float64, visto en una tabla:\")\n",
    "eph.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05c114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Definimos que variables no son categoricas\n",
    "categoricas = ['CH03', 'CH04', 'CH07', 'CH08', 'CH09', 'CH10', 'CH11', 'CH12', 'CH13', 'CH14', 'CH15', 'CH16', 'NIVEL_ED',\n",
    "               'ESTADO', 'CAT_OCUP', 'CAT_INAC', 'PP02C1', 'PP02C2', 'PP02C3', 'PP02C4', 'PP02C5', 'PP02C6', 'PP02C7', \n",
    "               'PP02C8', 'PP02E', 'PP02H', 'PP02I', 'PP03C', 'PP03G', 'PP03H', 'PP03I', 'PP03J', 'INTENSI', 'PP04A', \n",
    "               'PP04B1', 'PP04B3_MES', 'PP04B3_ANO', 'PP04B3_DIA', 'PP04C', 'PP04C99', 'PP04G', 'PP05B2_MES', 'PP05B2_ANO',\n",
    "               'PP05B2_DIA', 'PP05C_1', 'PP05C_2', 'PP05C_3', 'PP05E', 'PP05F', 'PP05H', 'PP06A', 'PP06E', 'PP06H', 'PP07A',\n",
    "               'PP07C', 'PP07D', 'PP07E', 'PP07F1', 'PP07F2', 'PP07F3', 'PP07F4', 'PP07F5', 'PP07G1', 'PP07G2', 'PP07G3', \n",
    "               'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K', 'PP09A', 'PP09B', 'PP09C', 'IV1', 'IV2', 'IV3', \n",
    "               'IV4', 'IV5', 'IV6', 'IV7', 'IV8', 'IV9', 'IV10', 'IV11', 'IV12_1', 'IV12_2', 'IV12_3', 'II1', 'II2', 'II3', \n",
    "               'II3_1', 'II4_1', 'II4_2', 'II4_3', 'II5', 'II5_1', 'II6', 'II6_1', 'II7', 'II8', 'II9', 'V1', 'V2', 'V21', \n",
    "               'V22', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17',\n",
    "               'V18', 'V19_A', 'V19_B', 'IX_TOT', 'IX_MEN10', 'IX_MAYEQ10', 'VII1_1', 'VII1_2', 'VII2_1', 'VII2_2',\n",
    "               'VII2_3', 'VII2_4']\n",
    "#Me fijo cuales fueron borradas mas arriba por tener muchos NAs\n",
    "no_borradas = pd.Index(categoricas).difference(eph.columns).tolist()\n",
    "#las saco\n",
    "categoricas = pd.Index(categoricas).difference(no_borradas).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b96e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Las convertimos a categoricas\n",
    "eph_d=eph.copy()\n",
    "eph_d[categoricas] = eph_d[categoricas].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazamos NAs de categoricas por -1\n",
    "for col in categoricas:\n",
    "    eph_d[col]=eph_d[col].cat.add_categories(['-1'])\n",
    "    eph_d[col]=eph_d[col].fillna('-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_llenar_con_ceros = eph.columns[eph.isnull().any()]\n",
    "eph[columnas_llenar_con_ceros]=eph[columnas_llenar_con_ceros].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d856b",
   "metadata": {},
   "source": [
    "PARA QUE ES eph_d ??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06458d",
   "metadata": {},
   "source": [
    "4) Creamos 2 nuevas variables:\n",
    " _hab= cociente entre miembros del hogar y cantidad de habitaciones\n",
    " _niños= proporción de niños menores a 10 años en el hogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos hab\n",
    "eph[\"hab\"]=eph[\"IX_TOT\"]/eph[\"IV2\"]\n",
    "\n",
    "#Definimos niños\n",
    "eph[\"niños\"]=eph[\"IX_MEN10\"]/eph[\"IX_TOT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59452f9c",
   "metadata": {},
   "source": [
    "5) Analizamos la correlación entre Nivel educativo (NIVEL_ED), Cantidad de miembros del hogar (IX_TOT) e Ingreso Total Familiar (ITF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ecc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gráfico: (IX_TOT;NIVEL_ED)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "ax.scatter(x=eph.IX_TOT, y=eph.ITF, alpha= 0.8)\n",
    "ax.set_xlabel('Miembros del Hogar')\n",
    "ax.set_ylabel('Ingreso total Familiar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_corr=eph[[\"NIVEL_ED\", \"IX_TOT\", \"ITF\"]]\n",
    "sns.set(rc = {'figure.figsize':(8,4)})\n",
    "sns.heatmap(df_corr.corr(), vmin = -1, vmax = +1, annot = True, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc3be8",
   "metadata": {},
   "source": [
    "En el primer gráfico observamos que las familias de mayores ingresos están compuestas por 3 o 4 miembros. A partir de esa cantidad de miembros el ingreso de la familia disminuye a mayor cantidad de miembros, haciendo que en la matriz de correlación observemos una correlación negativa entre ingreso familiar y cantidad de miembros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae17e75",
   "metadata": {},
   "source": [
    "6) \n",
    "-Agregamos la columna adulto_equiv y adulto_equiv_hogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_equiv = pd.read_excel('tabla_adulto_equiv.xlsx')\n",
    "\n",
    "df_equiv.loc[1,\"Edad\"]=1\n",
    "\n",
    "\n",
    "for i in range(0,18):\n",
    "    df_equiv.loc[i,\"Edad\"]= i \n",
    "\n",
    "for i in range(18,30): #18-29 años 0.76/1.02\n",
    "    df_equiv.loc[i]= [i,0.76,1.02] \n",
    "\n",
    "for i in range(30,46): #30-45 años 0.77/1\n",
    "    df_equiv.loc[i]= [i,0.77,1] \n",
    "\n",
    "for i in range(46,61): #46-60 años 0.76/1\n",
    "    df_equiv.loc[i]= [i,0.76,1] \n",
    "\n",
    "for i in range(61,76): #61-75 años 0.67/0.83\n",
    "    df_equiv.loc[i]= [i,0.67,0.83]\n",
    "\n",
    "for i in range(76,106): #18-29 años 0.63/0.74. La edad max en la EPH es 105 años. \n",
    "    df_equiv.loc[i]= [i,0.63,0.74] \n",
    "\n",
    "df_equiv = df_equiv.rename(columns={'Edad':'CH06'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2dea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eph= eph.merge(df_equiv, on='CH06')\n",
    "\n",
    "for i in range(len(df_eph)):\n",
    "    if df_eph.loc[i,\"CH04\"]==1:\n",
    "        df_eph.loc[i,\"adulto_equiv\"]=df_eph.loc[i,\"Varones\"]\n",
    "    elif df_eph.loc[i,\"CH04\"]==2:\n",
    "        df_eph.loc[i,\"adulto_equiv\"]=df_eph.loc[i,\"Mujeres\"]\n",
    "\n",
    "\n",
    "df_eph.drop ([\"Varones\",\"Mujeres\"] , axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882adc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum=df_eph.groupby(['CODUSU', 'NRO_HOGAR'])[['adulto_equiv']].agg('sum')\n",
    "df_sum.reset_index( inplace = True )\n",
    "df_sum = df_sum.rename(columns={'adulto_equiv':'ad_equiv_hogar'})\n",
    "\n",
    "df_eph=df_eph.merge(df_sum, on=[\"CODUSU\", \"NRO_HOGAR\"])\n",
    "df_eph=df_eph.sort_values(by=\"CODUSU\")\n",
    "\n",
    "#El data frame relevante ahora es df_eph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772063a",
   "metadata": {},
   "source": [
    "-Dividimos la base en respondieron y no respondieron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98127498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos las obs que respondieron en el df 'df_respondieron':\n",
    "df_respondieron = df_eph.drop(df_eph[df_eph['ITF']<=0].index)\n",
    "df_respondieron.reset_index(inplace=True, drop=True)\n",
    "\n",
    "##Guardamos las obs que no respondieron en el df 'df_norespondieron':\n",
    "df_norespondieron= df_eph.drop(df_eph[df_eph['ITF']>0].index) \n",
    "df_norespondieron.reset_index( inplace = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bca3e6",
   "metadata": {},
   "source": [
    "-Agregamos la variable \"ingreso_necesario\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_respondieron[\"ingreso_necesario\"]=df_respondieron[\"ad_equiv_hogar\"]*(27197.64) #$27197.64 es la CBA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac35ee8",
   "metadata": {},
   "source": [
    "7) Agregamos la variable \"pobre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_respondieron['Pobre'] = np.where(df_respondieron['ITF'] < df_respondieron['ingreso_necesario'], 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612b094",
   "metadata": {},
   "source": [
    "8) Calculamos la tasa de hogares pobres para GBA, utilizando el ponderador PONDICH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Me quedo solo con GBA\n",
    "df_respondieron_GBA =df_respondieron[df_respondieron['REGION']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d820fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creamos base con solo una observacion por hogar\n",
    "eph_hogares=df_respondieron_GBA.groupby(['CODUSU','NRO_HOGAR']).agg({\"Pobre\" : \"mean\",\"PONDIH\" : \"sum\"})\n",
    "#Sumamos cuanto hay de cada uno. \n",
    "sumas=eph_hogares.groupby(\"Pobre\").PONDIH.sum().to_frame()\n",
    "eph_porcentajes = sumas.apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))\n",
    "print(eph_porcentajes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b80a2",
   "metadata": {},
   "source": [
    "Notese que el porcentaje de pobres utilizando este metodo solo para REGION del GBA (distinto era si usabamos AGLOMERADO 33, partidos del GBA) es de 34.34%, mas de 4 p.p. por debajo del 39,2% reportado por INDEC considerando a nivel individual. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022fcd3",
   "metadata": {},
   "source": [
    "## PARTE II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6929a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698ce87",
   "metadata": {},
   "source": [
    "1) Definimos la función evalua_metodo. Qué toma como inputs las bases de train y test para X e Y y el modelo deseado (LogisticRegression, LinearDiscriminantAnalysis, KNeighborsClassifier,DecisionTreeClassifier,BaggingClassifier,RandomForestClassifier,AdaBoostClassifier,svm) con su hiperparámetro. Y devuelve una colección con las métricas correspondientes a cada modelo, entre las que se encuentran:  ECM, Auc, Presición,  Verdaderos Positivos, Verdaderos Negativos, Falsos Positivos y Falsos Negativos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d124e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_metodo(X_train, X_test, y_train, y_test, model, hiperparam, lasso=1, hiperparam_2=3):\n",
    "    '''    \n",
    "    Toma como parametros las bases de train y test y el modelo deseado de clasificación (LogisticRegression, LinearDiscriminantAnalysis,\n",
    "    KNeighborsClassifier,DecisionTreeClassifier,BaggingClassifier,RandomForestClassifier,AdaBoostClassifier,svm) y el valor de los hiperparámetros de cada modelo.  \n",
    "    Devuelve las métricas: Accuracy, AUC y ECM.\n",
    "    Para hacer elastic net reducir el valor predeterminado del parámetro lasso, lasso=0 indica Ridge. \n",
    "    '''\n",
    "    if model==DecisionTreeClassifier:\n",
    "        y = model(max_depth = hiperparam).fit(X_train, y_train)\n",
    "    elif model== LogisticRegression:\n",
    "        y = model(C=1/hiperparam, penalty=\"elasticnet\", solver=\"saga\", l1_ratio=lasso, tol=0.01).fit(X_train, y_train).fit(X_train, y_train) # l1_ratio=1 se hace Lasso | l1_ratio=0 se hace Ridge | 0 <l1_ratio< 1 combinación de ambos en elastic net.      \n",
    "    elif model==BaggingClassifier:\n",
    "        y=model(n_estimators=hiperparam, max_samples=200, random_state=0).fit(X_train, y_train) #base_estimator=None(hace DecicionTreeClass.); bootstrap=True\n",
    "    elif model==RandomForestClassifier:\n",
    "        y=model(n_estimators=hiperparam, max_samples=200, max_features=hiperparam_2, random_state=0).fit(X_train, y_train)\n",
    "    elif model== AdaBoostClassifier:\n",
    "        y=model(n_estimators=hiperparam, random_state=0).fit(X_train, y_train)\n",
    "    elif model==svm:\n",
    "        y=svm.SVC(C=1/hiperparam).fit(X_train, y_train)\n",
    "    if model==KNeighborsClassifier:\n",
    "        y = model(hiperparam).fit(X_train, y_train)\n",
    "    else:\n",
    "        y = model().fit(X_train, y_train)\n",
    "\n",
    "    y_pred = y.predict(X_test)\n",
    "\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, tresholds = roc_curve(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    ecm = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    df_metrics = pd.DataFrame({'modelo': [model], 'accuracy': [accuracy], 'auc':[auc], 'ecm':[ecm], 'VP': [matriz_confusion[1,1]], 'FP': [matriz_confusion[0,1]], 'VN':matriz_confusion[0,0], 'FN':matriz_confusion[1,0]})\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db1480",
   "metadata": {},
   "source": [
    "2) Definimos la función cross_validation(model, x, y, hiperparam, hiperparam_2, k_cv, standard=false) que toma como input el modelo desado (LogisticRegression, LinearDiscriminantAnalysis, KNeighborsClassifier,DecisionTreeClassifier,BaggingClassifier,RandomForestClassifier,AdaBoostClassifier,svm)las bases X e Y,el valor del hiperparámetro del modelo, el valor del segundo hiperparámetro del modelo y la cantidad de particiones 'k' para hacer CV. Standard=True indica si se quiere estandarizar las variables para hacer CV. La función devuelve la media de los ECM de las k estimaciones realizadas por el método de cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, x, y, hiperparam ,lasso=1, hiperparam_2=3, k=5, standard=False):\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    ecm = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):   \n",
    "        x_train, x_test = x.iloc[list(train_index)], x.iloc[list(test_index)]  \n",
    "        y_train, y_test = y.iloc[list(train_index)], y.iloc[list(test_index)]\n",
    "            \n",
    "        if standard==True:\n",
    "            # Estandarizamos las observaciones de entrenamiento\n",
    "            x_train = pd.DataFrame(sc.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\n",
    "            # Estandarizamos las observaciones de test\n",
    "            x_test = pd.DataFrame(sc.transform(x_test),index=x_test.index, columns=x_test.columns)\n",
    "\n",
    "        ecm2 = evalua_metodo(x_train, x_test, y_train, y_test, model, hiperparam, lasso=lasso, hiperparam_2=hiperparam_2)\n",
    "        ecm3 = ecm2['ecm'][0]\n",
    "        ecm.append(ecm3)\n",
    "        \n",
    "    final_ecm = statistics.mean(ecm)\n",
    "\n",
    "    return final_ecm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8e92b",
   "metadata": {},
   "source": [
    "3) Definimos la función evalua_config(model,hiperparam, x, y,k_cv,hiperparam_2, two_hiperparam, standard) que toma cómo inputs un modelo, una collección de valores de hiperparámetros (pueden ser una matriz o lista),las bases X e Y, el k de cross-validation y una colección de valores para un segundo hiperparámetro. two_hiperparam=True indica que se quiere optimar el modelo eligiendo 2 hiperparámetros. Standard=True indica que se estandarizan las varibales al hacer CV. Con esta información la función se encarga de calcular el/los hiperparámetros que minimiza/n el ECM por Cross-validation. Retorna el/los valor/es de hiperparámetro/s óptimo/s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_config (model,hiperparam, x, y, k=5, hiperparam_2=1, two_hiperparam=False, standard=False, lasso=1):\n",
    "    '''\n",
    "    Toma cómo input un modelo, una serie de valores de hiperparámetros para ese modelo y el K para CV.\n",
    "    Devuelve el hiperparámetro que minimiza el ECM por CV. Standard=True indica que se hace CV con variables standarizadas.\n",
    "    Para Logistic Regression optimiza el lambda y el parámetro de elastic net. \n",
    "    '''\n",
    "    \n",
    "    ret={}\n",
    "    lasso=np.arange(0,lasso,0.1).tolist() #lista de parámetros de elastic net\n",
    "    if model==LogisticRegression:\n",
    "        for elem in hiperparam:\n",
    "            for elem_2 in lasso:\n",
    "                ecm=cross_validation(model,x,y,elem, lasso=elem_2, k=k, standard=standard)\n",
    "                ret[(elem,elem_2)]= ecm\n",
    "        \n",
    "        return min(ret, key=ret.get) \n",
    "    \n",
    "    if two_hiperparam==True:\n",
    "        for elem in hiperparam:\n",
    "            for elem_2 in hiperparam_2:\n",
    "                ecm=cross_validation(model,x,y,elem,hiperparam_2=elem_2,k=k, standard=standard)\n",
    "                ret[(elem,elem_2)]= ecm\n",
    "                \n",
    "\n",
    "        \n",
    "        return min(ret, key=ret.get)     \n",
    "    \n",
    "    elif two_hiperparam==False:\n",
    "       for elem in hiperparam:     \n",
    "            ecm=cross_validation(model,x, y,elem,k=k, standard=standard)\n",
    "            ret[elem]= ecm \n",
    "       return min(ret, key=ret.get) #Devuelve el hiperarámetro que minimiza el ecm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e580321",
   "metadata": {},
   "source": [
    "4) Definimos la función evalua_metodos(hiperparam_max, x_train, x_test , y_train, y_test, x, y, k_cv, hiperparam_2) que toma cómo inputs el k de Cross-Validation, el valor máximo a testear del hiperparámetro, las bases de train y test de X e Y, y las bases completas de X e Y. La función se encarga de calcular los modelos: KNN, Regresión logistica, Análisis Lineal de Discriminante, Árbol de Decisión, Ramdom Forest, Boosting, Bagging y SVM. Y devuelve una tabla con las métricas correspondientes a cada modelo, entre las que se encuentran: hiperparámetro elegido por CV, ECM, Auc, Presición,  Verdaderos Positivos, Verdaderos Negativos, Falsos Positivos y Falsos Negativos. \n",
    "Con el valor del hiperparm_max lo que hace es generar una lista desde 1 hasta ese valor máximo, avanzando de a 3 unidades (lo establecimos así para reducir tiempo de computo, pero se podría establecer cualquier otro número entero).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_multiples_metodos (hiperparm_max, x_train, x_test , y_train, y_test, k=5, hiperparam_2_max=5): \n",
    "    \n",
    "\n",
    "\n",
    "    modelos = [[LogisticRegression,\"LogisticRegression\"],[KNeighborsClassifier,\"KNeighborsClassifier\"],[LinearDiscriminantAnalysis,\"LinearDiscriminantAnalysis\"], [DecisionTreeClassifier,\"DecisionTreeClassifier\"],[BaggingClassifier,\"BaggingClassifier\"],[AdaBoostClassifier,\"AdaBoostClassifier\"],[svm,\"svm\"],[RandomForestClassifier,\"RandomForestClassifier\"]]\n",
    "\n",
    "    hiperparam= np.arange(1,hiperparm_max,1).tolist() #Para reducir el tiempo de computo establecemos que los hiperparámetros a testear son números enteros. \n",
    "\n",
    "    matriz = pd.DataFrame(columns=[\"Modelo\", \"Hiperparametro 1\",\"Hiperparametro 2\",\"ECM\", \"AUC\",'Accuracy', 'VP', 'VN', 'FP', 'FN'])\n",
    "    print(matriz)\n",
    "    for modelo in modelos:\n",
    "        \n",
    "        if modelo[0]==RandomForestClassifier:\n",
    "            hiperparam_2=np.arange(1,hiperparam_2_max,1).tolist()\n",
    "            hiper_opt = evalua_config(modelo[0],hiperparam, x_train, y_train, k=k, hiperparam_2=hiperparam_2,two_hiperparam=True) \n",
    "            print(\"hola sot hiper opt rf\",hiper_opt)\n",
    "            metricas= evalua_metodo(x_train, x_test , y_train, y_test , modelo[0], hiper_opt[0], hiperparam_2=hiper_opt[1]) \n",
    "            results = [modelo[1], hiper_opt[0], hiper_opt[1] ,metricas['ecm'][0], metricas['auc'][0], metricas['accuracy'][0], metricas['VP'][0],metricas['VN'][0], metricas['FP'][0],metricas['FN'][0]]\n",
    "            matriz.loc[len(matriz)] = results\n",
    "            print(\"termine con RF\")\n",
    "        if modelo[0]==LogisticRegression:\n",
    "            hiper_opt = evalua_config(modelo[0],hiperparam, x_train, y_train,lasso=1,k=k, standard=True ) \n",
    "            metricas= evalua_metodo(x_train, x_test , y_train, y_test , modelo[0], hiper_opt[0],lasso=hiper_opt[1]) \n",
    "            results = [modelo[1], hiper_opt[0], hiper_opt[1] ,metricas['ecm'][0], metricas['auc'][0], metricas['accuracy'][0], metricas['VP'][0],metricas['VN'][0], metricas['FP'][0],metricas['FN'][0]]\n",
    "            matriz.loc[len(matriz)] = results\n",
    "            print(\"termine con LOGISTIC REG\")\n",
    "        else:\n",
    "            hiper_opt = evalua_config(modelo[0],hiperparam, x_train, y_train, k=k, standard=True ) \n",
    "            metricas= evalua_metodo(x_train, x_test , y_train, y_test , modelo[0], hiper_opt)\n",
    "            print(\"HOLAAAA\", hiper_opt)\n",
    "            results = [modelo[1], hiper_opt,\"NA\", metricas['ecm'][0], metricas['auc'][0], metricas['accuracy'][0], metricas['VP'][0],metricas['VN'][0], metricas['FP'][0],metricas['FN'][0]]\n",
    "            matriz.loc[len(matriz)] = results\n",
    "            print(\"termine con\", modelo[1])\n",
    "    \n",
    "    return(matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664f908",
   "metadata": {},
   "source": [
    "## PARTE III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bf806",
   "metadata": {},
   "source": [
    "1) Eliminamos todas variables de ingreso. Especificamos a que tipo de ingreso esta relacionada cada una. Dado que borramos algunas por tener demasiados NAs, borramos obviamente las no previamente decartadas, sacando la diferencia entre las variables definidas y las ya borradas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc07742",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ingreso = [\n",
    "    \"ITF\",\"DECIFR\",\"IDECIFR\",\"RDECIFR\",\"GDECIFR\", \"PDECIFR\",\"ADECIFR\", #Ingreso total familiar\n",
    "    \"IPCF\", \"DECCFR\", \"IDECCFR\", \"RDECCFR\", \"GDECCFR\", \"PDECCFR\", \"ADECCFR\", \"PONDIH\", #Ingreso per capita familiar\n",
    "    \"PP06A\", \"PP06C\", \"PP06D\", \"PP06E\", \"PP06H\", #Ingreso de las ocupaciones principales de independientes\n",
    "    \"PP08D1\", \"PP08D4\", \"PP08F1\", \"PP08F2\", \"PP08J1\", \"PP08J2\", \"PP08J3\",#Ingreso de las ocupaciones principales de asalariados\n",
    "    \"P21\", \"DECOCUR\", \"IDECOCUR\", \"RDECOCUR\", \"GDECOCUR\", \"PDECOCUR\", \"ADECOCUR\", \"PONDIIO\", #Ingreso de la ocupacion principal\n",
    "    \"TOT_P12\", #Ingreso de otras ocupaciones\n",
    "    \"P47T\", \"DECINDR\", \"IDECINDR\", \"RDECINDR\", \"GDECINDR\", \"PDECINDR\", \"ADECINDR\", \"PONDII\",#Ingreso total individual\n",
    "    \"V2_M\", \"V3_M\", \"V4_M\", \"V5_M\", \"V8_M\", \"V9_M\", \"V10_M\", \"V11_M\", \"V12_M\", \"V18_M\", \"V19_AM\", \"V21_M\", \"T_VI\", #Ingresos no laborales\n",
    "    \"adulto_equiv\", \"ad_equiv_hogar\",  \"ingreso_necesario\"#Equivalentes e ingreso necesario\n",
    "]\n",
    "#Me fijo cuales fueron borradas mas arriba por tener muchos NAs\n",
    "borradas = pd.Index(variables_ingreso).difference(df_norespondieron.columns).tolist()\n",
    "#las saco\n",
    "variables_ingreso_no_borradas = pd.Index(variables_ingreso).difference(borradas).tolist()\n",
    "\n",
    "#Eliminamos\n",
    "df_respondieron.drop(columns = variables_ingreso_no_borradas)\n",
    "df_respondieron.drop(columns = ['ingreso_necesario']) #Esta solo en df_respondieron\n",
    "df_norespondieron.drop(columns = variables_ingreso_no_borradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95e872",
   "metadata": {},
   "source": [
    "EL BLOQUE ANTERIOR DA UN PRINT LARGUISIMO DE TODA LA BASE NOSE POR QUE!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f58ca9",
   "metadata": {},
   "source": [
    "Definimos y'' (variable dependiente) igual a sí es pobre o no. X (matriz de variables independientes) es el resto de variables que quedarin en la base, y agregamos el ventor de constantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recordemos que tenemos 2 data base relevantes: df_respondieron y df_norespondieron\n",
    "#Pasamos a float la variable MAS_500 que es de tipo String:\n",
    "for i in range(len(df_respondieron)):\n",
    "    if df_respondieron.loc[i,\"MAS_500\"] == \"S\" :\n",
    "        df_respondieron.loc[i,\"MAS_500\"]= 1\n",
    "    elif df_respondieron.loc[i,\"MAS_500\"] == \"N\":\n",
    "        df_respondieron.loc[i,\"MAS_500\"]=0 \n",
    "\n",
    "for i in range(len(df_norespondieron)):\n",
    "    if df_norespondieron.loc[i,\"MAS_500\"] == \"S\" :\n",
    "        df_norespondieron.loc[i,\"MAS_500\"]= 1\n",
    "    elif df_norespondieron.loc[i,\"MAS_500\"] == \"N\":\n",
    "        df_norespondieron.loc[i,\"MAS_500\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bedd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos CODUSU Y CH05, que son strings\n",
    "df_norespondieron.drop ([\"CODUSU\", \"CH05\"] , axis =1 , inplace = True)\n",
    "df_respondieron.drop ([\"CODUSU\", \"CH05\"] , axis =1 , inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc445ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_respondieron['Pobre'] \n",
    "X=df_respondieron.loc[:, df_respondieron.columns != 'Pobre']\n",
    "X['Constante']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b20ed",
   "metadata": {},
   "source": [
    "2) Corremos la funcion evalua_multiples_metodos con la base respondieron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92681487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "evalua_multiples_metodos (5, X_train, X_test , y_train, y_test,k=3,hiperparam_2_max=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bffcb00b7ac34bf829d2aec2cc2dc2ffd9f66546b23bd1c848b28266b45973a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
